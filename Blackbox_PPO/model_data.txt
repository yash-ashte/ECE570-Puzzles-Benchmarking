blackbox unrewarded PPo
Evaluation:
Mean reward: 100.0, std: 0.0
Mean episode length: 2782.6666666666665, std: 1582.7436796763889
Number of puzzles solved: 3 out of 3

Args: Namespace(puzzle='blackbox', size=(128, 128), arg='w2h2m2M2', headless=False, allowundo=False, timesteps=50000, timelimit=10000, algorithm='PPO', obs_type='puzzle_state', seed=0, max_state_repeats=1000000)
log_dir = ./results/monitor/PPO_50000/blackbox_w2h2m2M2_noundo_puzzle_state/
model_dir = ./results/models/PPO_50000/blackbox_w2h2m2M2_puzzle_state_reward/
Using cpu device
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 1 Summary:
Actions taken: {np.int64(4): 205, np.int64(1): 210, np.int64(2): 190, np.int64(3): 168, np.int64(0): 185}
Num timesteps: 1000
Best mean length: inf - Last mean length per episode: 958.00
Saving new best model to ./results/monitor/PPO_50000/blackbox_w2h2m2M2_noundo_puzzle_state/best_model_blackbox
Num timesteps: 2000
Best mean length: 958.00 - Last mean length per episode: 958.00
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 958      |
|    ep_rew_mean     | -1.1e+03 |
| time/              |          |
|    fps             | 61       |
|    iterations      | 1        |
|    time_elapsed    | 33       |
|    total_timesteps | 2048     |
---------------------------------
Num timesteps: 3000
Best mean length: 958.00 - Last mean length per episode: 958.00
Num timesteps: 4000
Best mean length: 958.00 - Last mean length per episode: 958.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 958         |
|    ep_rew_mean          | -1.1e+03    |
| time/                   |             |
|    fps                  | 60          |
|    iterations           | 2           |
|    time_elapsed         | 67          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011610266 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | -0.00577    |
|    learning_rate        | 0.0003      |
|    loss                 | 32.8        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 183         |
-----------------------------------------
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 2 Summary:
Actions taken: {np.int64(2): 669, np.int64(3): 648, np.int64(0): 709, np.int64(1): 867, np.int64(4): 924}
Num timesteps: 5000
Best mean length: 958.00 - Last mean length per episode: 2387.50
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 3 Summary:
Actions taken: {np.int64(0): 71, np.int64(2): 87, np.int64(1): 73, np.int64(4): 92, np.int64(3): 61}
Num timesteps: 6000
Best mean length: 958.00 - Last mean length per episode: 1719.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.72e+03    |
|    ep_rew_mean          | -1.98e+03   |
| time/                   |             |
|    fps                  | 60          |
|    iterations           | 3           |
|    time_elapsed         | 101         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.015632527 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.0143      |
|    learning_rate        | 0.0003      |
|    loss                 | 55.1        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 196         |
-----------------------------------------
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 4 Summary:
Actions taken: {np.int64(4): 496, np.int64(2): 355, np.int64(0): 413, np.int64(3): 230, np.int64(1): 330}
Num timesteps: 7000
Best mean length: 958.00 - Last mean length per episode: 1745.75
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 5 Summary:
Actions taken: {np.int64(3): 105, np.int64(0): 197, np.int64(2): 140, np.int64(4): 254, np.int64(1): 138}
Num timesteps: 8000
Best mean length: 958.00 - Last mean length per episode: 1563.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.56e+03    |
|    ep_rew_mean          | -1.79e+03   |
| time/                   |             |
|    fps                  | 60          |
|    iterations           | 4           |
|    time_elapsed         | 135         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.013721403 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | -0.0252     |
|    learning_rate        | 0.0003      |
|    loss                 | 40.3        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 159         |
-----------------------------------------
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 6 Summary:
Actions taken: {np.int64(0): 193, np.int64(2): 176, np.int64(4): 192, np.int64(3): 103, np.int64(1): 90}
Num timesteps: 9000
Best mean length: 958.00 - Last mean length per episode: 1428.50
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 7 Summary:
Actions taken: {np.int64(4): 289, np.int64(2): 235, np.int64(1): 148, np.int64(0): 286, np.int64(3): 147}
Num timesteps: 10000
Best mean length: 958.00 - Last mean length per episode: 1382.29
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.38e+03    |
|    ep_rew_mean          | -1.59e+03   |
| time/                   |             |
|    fps                  | 60          |
|    iterations           | 5           |
|    time_elapsed         | 170         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.013249529 |
|    clip_fraction        | 0.0967      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.182       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.4        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00648    |
|    value_loss           | 131         |
-----------------------------------------
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 8 Summary:
Actions taken: {np.int64(1): 148, np.int64(4): 184, np.int64(3): 103, np.int64(2): 134, np.int64(0): 219}
Num timesteps: 11000
Best mean length: 958.00 - Last mean length per episode: 1308.00
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 9 Summary:
Actions taken: {np.int64(0): 378, np.int64(2): 240, np.int64(1): 139, np.int64(4): 350, np.int64(3): 153}
Num timesteps: 12000
Best mean length: 958.00 - Last mean length per episode: 1302.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.3e+03     |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 60          |
|    iterations           | 6           |
|    time_elapsed         | 204         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012736044 |
|    clip_fraction        | 0.0909      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | -0.00101    |
|    learning_rate        | 0.0003      |
|    loss                 | 34.1        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 121         |
-----------------------------------------
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 10 Summary:
Actions taken: {np.int64(0): 182, np.int64(4): 409, np.int64(2): 212, np.int64(3): 106, np.int64(1): 143}
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 11 Summary:
Actions taken: {np.int64(1): 24, np.int64(2): 26, np.int64(3): 17, np.int64(4): 37, np.int64(0): 31}
Num timesteps: 13000
Best mean length: 958.00 - Last mean length per episode: 1173.73
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 12 Summary:
Actions taken: {np.int64(2): 191, np.int64(4): 273, np.int64(1): 157, np.int64(0): 216, np.int64(3): 88}
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 13 Summary:
Actions taken: {np.int64(0): 30, np.int64(4): 51, np.int64(1): 27, np.int64(2): 35, np.int64(3): 17}
Num timesteps: 14000
Best mean length: 958.00 - Last mean length per episode: 1076.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.08e+03     |
|    ep_rew_mean          | -1.24e+03    |
| time/                   |              |
|    fps                  | 60           |
|    iterations           | 7            |
|    time_elapsed         | 238          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0129618645 |
|    clip_fraction        | 0.0748       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.51        |
|    explained_variance   | 0.00348      |
|    learning_rate        | 0.0003       |
|    loss                 | 24.4         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00515     |
|    value_loss           | 109          |
------------------------------------------
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 14 Summary:
Actions taken: {np.int64(2): 94, np.int64(4): 136, np.int64(1): 49, np.int64(3): 35, np.int64(0): 65}
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 15 Summary:
Actions taken: {np.int64(3): 73, np.int64(1): 69, np.int64(4): 73, np.int64(0): 82, np.int64(2): 97}
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 16 Summary:
Actions taken: {np.int64(4): 16, np.int64(2): 6, np.int64(3): 4, np.int64(0): 6, np.int64(1): 2}
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 17 Summary:
Actions taken: {np.int64(2): 25, np.int64(3): 12, np.int64(4): 34, np.int64(0): 29, np.int64(1): 20}
Num timesteps: 15000
Best mean length: 958.00 - Last mean length per episode: 877.82
Saving new best model to ./results/monitor/PPO_50000/blackbox_w2h2m2M2_noundo_puzzle_state/best_model_blackbox
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 18 Summary:
Actions taken: {np.int64(1): 103, np.int64(0): 201, np.int64(4): 177, np.int64(2): 195, np.int64(3): 105}
Num timesteps: 16000
Best mean length: 877.82 - Last mean length per episode: 872.44
Saving new best model to ./results/monitor/PPO_50000/blackbox_w2h2m2M2_noundo_puzzle_state/best_model_blackbox
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 19 Summary:
Actions taken: {np.int64(4): 140, np.int64(2): 62, np.int64(3): 39, np.int64(0): 111, np.int64(1): 90}
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 20 Summary:
Actions taken: {np.int64(4): 56, np.int64(3): 19, np.int64(2): 22, np.int64(1): 14, np.int64(0): 17}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 814         |
|    ep_rew_mean          | -932        |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 8           |
|    time_elapsed         | 273         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.012105409 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | -0.0954     |
|    learning_rate        | 0.0003      |
|    loss                 | 49          |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 107         |
-----------------------------------------
Num timesteps: 17000
Best mean length: 872.44 - Last mean length per episode: 813.70
Saving new best model to ./results/monitor/PPO_50000/blackbox_w2h2m2M2_noundo_puzzle_state/best_model_blackbox
Num timesteps: 18000
Best mean length: 813.70 - Last mean length per episode: 813.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 814         |
|    ep_rew_mean          | -932        |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 9           |
|    time_elapsed         | 307         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.021095261 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | -0.0227     |
|    learning_rate        | 0.0003      |
|    loss                 | 44.5        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 116         |
-----------------------------------------
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 21 Summary:
Actions taken: {np.int64(4): 1091, np.int64(0): 348, np.int64(1): 139, np.int64(2): 438, np.int64(3): 174}
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 22 Summary:
Actions taken: {np.int64(4): 119, np.int64(1): 22, np.int64(2): 47, np.int64(3): 34, np.int64(0): 22}
Num timesteps: 19000
Best mean length: 813.70 - Last mean length per episode: 850.36
Num timesteps: 20000
Best mean length: 813.70 - Last mean length per episode: 850.36
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 850          |
|    ep_rew_mean          | -972         |
| time/                   |              |
|    fps                  | 59           |
|    iterations           | 10           |
|    time_elapsed         | 341          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0130983535 |
|    clip_fraction        | 0.172        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | -0.00396     |
|    learning_rate        | 0.0003       |
|    loss                 | 7.53         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.0138      |
|    value_loss           | 50.1         |
------------------------------------------
Num timesteps: 21000
Best mean length: 813.70 - Last mean length per episode: 850.36
Num timesteps: 22000
Best mean length: 813.70 - Last mean length per episode: 850.36
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 850          |
|    ep_rew_mean          | -972         |
| time/                   |              |
|    fps                  | 59           |
|    iterations           | 11           |
|    time_elapsed         | 376          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0058409423 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.0462       |
|    learning_rate        | 0.0003       |
|    loss                 | 27.4         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00337     |
|    value_loss           | 68.3         |
------------------------------------------
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 23 Summary:
Actions taken: {np.int64(4): 2122, np.int64(2): 827, np.int64(1): 241, np.int64(0): 704, np.int64(3): 327}
Num timesteps: 23000
Best mean length: 813.70 - Last mean length per episode: 996.91
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 24 Summary:
Actions taken: {np.int64(4): 222, np.int64(3): 80, np.int64(2): 186, np.int64(0): 150, np.int64(1): 98}
Num timesteps: 24000
Best mean length: 813.70 - Last mean length per episode: 986.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 986         |
|    ep_rew_mean          | -1.13e+03   |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 12          |
|    time_elapsed         | 410         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.013453685 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.00102     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.5         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 25.6        |
-----------------------------------------
Num timesteps: 25000
Best mean length: 813.70 - Last mean length per episode: 986.04
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 25 Summary:
Actions taken: {np.int64(4): 1170, np.int64(2): 176, np.int64(3): 142, np.int64(0): 190, np.int64(1): 112}
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 26 Summary:
Actions taken: {np.int64(4): 15, np.int64(1): 2, np.int64(3): 1, np.int64(2): 3, np.int64(0): 3}
Num timesteps: 26000
Best mean length: 813.70 - Last mean length per episode: 979.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 980         |
|    ep_rew_mean          | -1.12e+03   |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 13          |
|    time_elapsed         | 444         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.019160762 |
|    clip_fraction        | 0.0913      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | -0.0238     |
|    learning_rate        | 0.0003      |
|    loss                 | 21.3        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 64.1        |
-----------------------------------------
Num timesteps: 27000
Best mean length: 813.70 - Last mean length per episode: 979.96
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 27 Summary:
Actions taken: {np.int64(4): 1634, np.int64(1): 97, np.int64(3): 148, np.int64(2): 430, np.int64(0): 198}
Num timesteps: 28000
Best mean length: 813.70 - Last mean length per episode: 1036.52
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 28 Summary:
Actions taken: {np.int64(4): 412, np.int64(3): 41, np.int64(2): 56, np.int64(0): 102, np.int64(1): 50}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -1.17e+03   |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 14          |
|    time_elapsed         | 479         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.009312835 |
|    clip_fraction        | 0.0654      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | -0.00517    |
|    learning_rate        | 0.0003      |
|    loss                 | 205         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.000172   |
|    value_loss           | 67.7        |
-----------------------------------------
Num timesteps: 29000
Best mean length: 813.70 - Last mean length per episode: 1023.11
Num timesteps: 30000
Best mean length: 813.70 - Last mean length per episode: 1023.11
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 29 Summary:
Actions taken: {np.int64(2): 322, np.int64(4): 1098, np.int64(0): 212, np.int64(3): 128, np.int64(1): 154}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.05e+03    |
|    ep_rew_mean          | -1.2e+03    |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 15          |
|    time_elapsed         | 514         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.003951196 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -0.000224   |
|    learning_rate        | 0.0003      |
|    loss                 | 12          |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.000606   |
|    value_loss           | 75.7        |
-----------------------------------------
Num timesteps: 31000
Best mean length: 813.70 - Last mean length per episode: 1053.83
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 30 Summary:
Actions taken: {np.int64(4): 500, np.int64(1): 49, np.int64(2): 52, np.int64(0): 71, np.int64(3): 31}
Num timesteps: 32000
Best mean length: 813.70 - Last mean length per episode: 1042.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.04e+03    |
|    ep_rew_mean          | -1.19e+03   |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 16          |
|    time_elapsed         | 548         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.010462848 |
|    clip_fraction        | 0.0874      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | -0.00338    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.646       |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00437    |
|    value_loss           | 41          |
-----------------------------------------
Num timesteps: 33000
Best mean length: 813.70 - Last mean length per episode: 1042.13
Num timesteps: 34000
Best mean length: 813.70 - Last mean length per episode: 1042.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.04e+03    |
|    ep_rew_mean          | -1.19e+03   |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 17          |
|    time_elapsed         | 582         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.006330643 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.796      |
|    explained_variance   | -0.0328     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.74        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00611    |
|    value_loss           | 42.5        |
-----------------------------------------
Num timesteps: 35000
Best mean length: 813.70 - Last mean length per episode: 1042.13
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 31 Summary:
Actions taken: {np.int64(1): 165, np.int64(4): 3122, np.int64(0): 266, np.int64(2): 369, np.int64(3): 194}
Num timesteps: 36000
Best mean length: 813.70 - Last mean length per episode: 1141.29
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 32 Summary:
Actions taken: {np.int64(4): 549, np.int64(1): 40, np.int64(2): 36, np.int64(0): 43, np.int64(3): 35}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.27e+03   |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 18          |
|    time_elapsed         | 616         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.004865814 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | -0.000545   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.46        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 3.77        |
-----------------------------------------
Num timesteps: 37000
Best mean length: 813.70 - Last mean length per episode: 1127.59
Num timesteps: 38000
Best mean length: 813.70 - Last mean length per episode: 1127.59
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 33 Summary:
Actions taken: {np.int64(4): 1765, np.int64(3): 119, np.int64(1): 159, np.int64(0): 165, np.int64(2): 162}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.17e+03    |
|    ep_rew_mean          | -1.31e+03   |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 19          |
|    time_elapsed         | 651         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.012082375 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.933      |
|    explained_variance   | -0.021      |
|    learning_rate        | 0.0003      |
|    loss                 | 43.9        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00642    |
|    value_loss           | 89.9        |
-----------------------------------------
Num timesteps: 39000
Best mean length: 813.70 - Last mean length per episode: 1165.24
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 34 Summary:
Actions taken: {np.int64(2): 35, np.int64(3): 31, np.int64(4): 487, np.int64(1): 43, np.int64(0): 54}
Num timesteps: 40000
Best mean length: 813.70 - Last mean length per episode: 1150.09
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 35 Summary:
Actions taken: {np.int64(4): 1230, np.int64(0): 107, np.int64(2): 85, np.int64(1): 84, np.int64(3): 57}
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.16e+03     |
|    ep_rew_mean          | -1.3e+03     |
| time/                   |              |
|    fps                  | 59           |
|    iterations           | 20           |
|    time_elapsed         | 685          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0038913125 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.798       |
|    explained_variance   | -0.0146      |
|    learning_rate        | 0.0003       |
|    loss                 | 2.36         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00349     |
|    value_loss           | 45           |
------------------------------------------
Num timesteps: 41000
Best mean length: 813.70 - Last mean length per episode: 1161.89
Num timesteps: 42000
Best mean length: 813.70 - Last mean length per episode: 1161.89
Num timesteps: 43000
Best mean length: 813.70 - Last mean length per episode: 1161.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.16e+03     |
|    ep_rew_mean          | -1.3e+03     |
| time/                   |              |
|    fps                  | 59           |
|    iterations           | 21           |
|    time_elapsed         | 719          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0031057491 |
|    clip_fraction        | 0.0449       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.759       |
|    explained_variance   | -0.000517    |
|    learning_rate        | 0.0003       |
|    loss                 | 112          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 88.3         |
------------------------------------------
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 36 Summary:
Actions taken: {np.int64(4): 2338, np.int64(1): 125, np.int64(3): 62, np.int64(0): 184, np.int64(2): 123}
Num timesteps: 44000
Best mean length: 813.70 - Last mean length per episode: 1208.28
Num timesteps: 45000
Best mean length: 813.70 - Last mean length per episode: 1208.28
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.21e+03     |
|    ep_rew_mean          | -1.35e+03    |
| time/                   |              |
|    fps                  | 59           |
|    iterations           | 22           |
|    time_elapsed         | 753          |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0031472677 |
|    clip_fraction        | 0.0381       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.675       |
|    explained_variance   | -0.000278    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.46         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00336     |
|    value_loss           | 4.08         |
------------------------------------------
Num timesteps: 46000
Best mean length: 813.70 - Last mean length per episode: 1208.28
Num timesteps: 47000
Best mean length: 813.70 - Last mean length per episode: 1208.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.21e+03    |
|    ep_rew_mean          | -1.35e+03   |
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 23          |
|    time_elapsed         | 787         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.007339591 |
|    clip_fraction        | 0.0636      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.775      |
|    explained_variance   | -0.016      |
|    learning_rate        | 0.0003      |
|    loss                 | 2.66        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 46.6        |
-----------------------------------------
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 37 Summary:
Actions taken: {np.int64(4): 3438, np.int64(1): 186, np.int64(2): 284, np.int64(3): 188, np.int64(0): 228}
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 38 Summary:
Actions taken: {np.int64(3): 19, np.int64(4): 111, np.int64(1): 14, np.int64(0): 11, np.int64(2): 10}
Num timesteps: 48000
Best mean length: 813.70 - Last mean length per episode: 1262.82
Num timesteps: 49000
Best mean length: 813.70 - Last mean length per episode: 1262.82
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.26e+03     |
|    ep_rew_mean          | -1.41e+03    |
| time/                   |              |
|    fps                  | 59           |
|    iterations           | 24           |
|    time_elapsed         | 822          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0072887777 |
|    clip_fraction        | 0.0881       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.691       |
|    explained_variance   | -0.00721     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.55         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00599     |
|    value_loss           | 4.24         |
------------------------------------------
Num timesteps: 50000
Best mean length: 813.70 - Last mean length per episode: 1262.82
CORRECT (+4.0) | PROGRESS (+0.3)

Episode 39 Summary:
Actions taken: {np.int64(4): 1659, np.int64(1): 116, np.int64(0): 136, np.int64(2): 90, np.int64(3): 107}
Num timesteps: 51000
Best mean length: 813.70 - Last mean length per episode: 1284.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.28e+03     |
|    ep_rew_mean          | -1.43e+03    |
| time/                   |              |
|    fps                  | 59           |
|    iterations           | 25           |
|    time_elapsed         | 856          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0035531833 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.769       |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.02         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00421     |
|    value_loss           | 51.7         |
------------------------------------------